{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹æ¶æ§‹èˆ‡è¨“ç·´\n",
    "è² è²¬åˆ‡åˆ†è¨“ç·´é›†ã€æ­£è¦åŒ–ã€è¨“ç·´æ¨¡å‹ ï¼Œä»¥é æ¸¬ **0056 (å…ƒå¤§é«˜è‚¡æ¯)** çš„æœªä¾†æ”¶ç›¤åƒ¹ã€‚\n",
    "1. **è³‡æ–™è™•ç†**: è¼‰å…¥å¤šæª”ç›¸é—œè‚¡ç¥¨æ•¸æ“šï¼Œä¸¦é€²è¡Œåˆä½µèˆ‡æ¸…æ´—ã€‚\n",
    "2. **ç‰¹å¾µå·¥ç¨‹**: è¨ˆç®— MA, RSI, MACD ç­‰é—œéµæŠ€è¡“æŒ‡æ¨™ã€‚\n",
    "3. **æ¨¡å‹è¨“ç·´**: çµåˆ XGBoost, Random Forest, LightGBM ä¸‰å¤§æ¨¡å‹é€²è¡ŒæŠ•ç¥¨æ±ºç­–ã€‚\n",
    "4. **æˆæ•ˆè©•ä¼°**: ä½¿ç”¨ RMSE èˆ‡ MAPE é©—è­‰æ¨¡å‹æº–ç¢ºåº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# 1. æ ¸å¿ƒæ•¸æ“šè™•ç†å¥—ä»¶ (Core Data Processing)\n",
    "# ========================================================\n",
    "import pandas as pd  # è³‡æ–™åˆ†æç¥å™¨ï¼šç”¨ä¾†æ“ä½œ DataFrameï¼Œå°±åƒ Python è£¡çš„ Excel\n",
    "import numpy as np   # æ•¸å­¸é‹ç®—åŸºçŸ³ï¼šè™•ç†é™£åˆ—ã€çŸ©é™£é‹ç®—ï¼Œé€Ÿåº¦æ¯” Python å…§å»ºåˆ—è¡¨å¿«\n",
    "\n",
    "# ========================================================\n",
    "# 2. è¦–è¦ºåŒ–ç¹ªåœ–å¥—ä»¶ (Visualization)\n",
    "# ========================================================\n",
    "import matplotlib.pyplot as plt # åŸºç¤ç¹ªåœ–åº«ï¼šç•«æŠ˜ç·šåœ–ã€è¨­å®šåœ–è¡¨å¤§å°èˆ‡è»¸æ¨™ç±¤\n",
    "import seaborn as sns           # é€²éšç¾åŒ–åº«ï¼šåŸºæ–¼ Matplotlibï¼Œç•«ç†±åŠ›åœ–æ›´æ¼‚äº®ï¼Œé¢¨æ ¼æ›´ç¾ä»£\n",
    "\n",
    "# è¨­å®š seaborn çš„é è¨­ç¾å­¸ (è®“åœ–è¡¨è‡ªå‹•è®Šæ¼‚äº®)\n",
    "sns.set_style(\"whitegrid\")      # èƒŒæ™¯åŠ æ ¼å­\n",
    "sns.set_context(\"notebook\")     # å­—é«”è¨­ç‚ºé©åˆé–±è®€çš„å¤§å°\n",
    "\n",
    "# ========================================================\n",
    "# 3. æ¨¡å‹è©•ä¼°æŒ‡æ¨™ (Evaluation Metrics)\n",
    "# ========================================================\n",
    "# ç”¨ä¾†å¹«æ¨¡å‹æ‰“åˆ†æ•¸çš„å·¥å…·\n",
    "# mean_squared_error: è¨ˆç®— RMSE (èª¤å·®é‡‘é¡) ç”¨\n",
    "# mean_absolute_percentage_error: è¨ˆç®— MAPE (èª¤å·®ç™¾åˆ†æ¯”) ç”¨\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# ========================================================\n",
    "# 4. è‡ªå®šç¾©æ¨¡çµ„ (Project Modules)\n",
    "# ========================================================\n",
    "# é€™æ˜¯æˆ‘å€‘å°ˆæ¡ˆè‡ªå·±å¯«çš„ä¸‰å€‹æ ¸å¿ƒæª”æ¡ˆ\n",
    "import data_loader          # æ­¥é©Ÿ 1: æ¬é‹å·¥ (è®€å–è³‡æ–™)\n",
    "import data_preprocessing   # æ­¥é©Ÿ 2: æ¸…æ½”å·¥ (æ´—è³‡æ–™)\n",
    "import feature_eng          # æ­¥é©Ÿ 3: å·¥ç¨‹å¸« (ç®—æŠ€è¡“æŒ‡æ¨™)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. è³‡æ–™æº–å‚™ (Data Preparation)\n",
    "\n",
    "åœ¨æ­¤éšæ®µï¼Œæˆ‘å€‘å‘¼å«è‡ªå®šç¾©çš„æ¨¡çµ„ä¾†åŸ·è¡Œæ¨™æº–åŒ–çš„è³‡æ–™è™•ç†æµç¨‹ (ETL)ï¼š\n",
    "* **Load**: è®€å–æ‰€æœ‰è‚¡ç¥¨ CSVã€‚\n",
    "* **Clean**: è™•ç†ç¼ºå€¼èˆ‡æ ¼å¼å•é¡Œã€‚\n",
    "* **Feature Engineering**: è‡ªå‹•è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ [Loader] é–‹å§‹è®€å–è³‡æ–™...\n",
      "âœ… è³‡æ–™è¼‰å…¥å®Œæˆ (å°šæœªæ¸…æ´—)ï¼å¤§å°ï¼š(852, 12)\n",
      "ğŸ§¹ [Preprocessing] é–‹å§‹æ¸…æ´—è³‡æ–™...\n",
      "âœ… è³‡æ–™æ¸…æ´—å®Œæˆï¼æ²’æœ‰ NaN äº†ã€‚\n",
      "ğŸ“Š [Feature Engineering] åµæ¸¬åˆ° 11 æ”¯è‚¡ç¥¨ï¼Œé–‹å§‹è¨ˆç®—å…¨å¥—æŠ€è¡“æŒ‡æ¨™...\n",
      "âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼ç›®å‰çš„æ¬„ä½æ•¸: 122\n",
      "è³‡æ–™æº–å‚™å®Œæˆï¼Œå½¢ç‹€: (852, 122)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_Vol_20'] = df[col].pct_change().rolling(window=20).std()\n",
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_BBP'] = (df[col] - lower_band) / (upper_band - lower_band + 1e-9)\n",
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_MA_5'] = ma_5\n",
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_RSI'] = calculate_rsi(df[col], period=14)\n",
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_MACD_DIF'] = dif\n",
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_MACD_DEM'] = dem\n",
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_MACD_OSC'] = osc\n",
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_Bias_5'] = (df[col] - ma_5) / ma_5\n",
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_Bias_20'] = (df[col] - ma_20) / ma_20\n",
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_Return_1'] = df[col].pct_change()\n",
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_Vol_20'] = df[col].pct_change().rolling(window=20).std()\n",
      "/Users/alice/Downloads/äººå·¥æ™ºæ…§/stock_project/feature_eng.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{prefix}_BBP'] = (df[col] - lower_band) / (upper_band - lower_band + 1e-9)\n"
     ]
    }
   ],
   "source": [
    "# é‡æ–°è·‘ä¸€æ¬¡æµç¨‹æ‹¿åˆ°è³‡æ–™\n",
    "df_raw = data_loader.load_and_merge_data()\n",
    "df_clean = data_preprocessing.clean_data(df_raw)\n",
    "df_features = feature_eng.add_technical_indicators(df_clean)\n",
    "\n",
    "print(\"è³‡æ–™æº–å‚™å®Œæˆï¼Œå½¢ç‹€:\", df_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. è³‡æ–™é›†åˆ‡åˆ† (Train-Test Split)\n",
    "\n",
    "ç‚ºäº†æ¨¡æ“¬çœŸå¯¦çš„æ™‚é–“åºåˆ—é æ¸¬å ´æ™¯ï¼Œæˆ‘å€‘æ¡ç”¨ **ã€Œæ™‚é–“åºåˆ—åˆ‡åˆ†æ³•ã€**ï¼Œè€Œééš¨æ©Ÿåˆ‡åˆ†ï¼š\n",
    "* **è¨“ç·´é›† (Train)**: ä½¿ç”¨éå»çš„æ­·å²æ•¸æ“šã€‚\n",
    "* **é©—è­‰é›† (Validation)**: ä¿ç•™æœ€å¾Œ 30 å¤©çš„æ•¸æ“šä½œç‚ºæ¨¡æ“¬è€ƒé¡Œï¼Œåš´æ ¼ç¦æ­¢æ¨¡å‹åœ¨è¨“ç·´æ™‚çœ‹åˆ°é€™äº›ç­”æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´é›†å¤§å°: (822, 120), é©—è­‰é›†å¤§å°: (30, 120)\n"
     ]
    }
   ],
   "source": [
    "# åˆ‡åˆ†è¨“ç·´é›†èˆ‡é©—è­‰é›† (ç‚ºäº†å¯¦é©—ï¼Œæˆ‘å€‘è‡ªå·±åˆ‡)\n",
    "# æˆ‘å€‘æ‹¿æœ€å¾Œ 30 å¤©ç•¶ä½œé©—è­‰é›† (æ¨¡æ“¬è€ƒ)ï¼Œå‰é¢ç•¶è¨“ç·´é›†\n",
    "test_days = 30\n",
    "train_df = df_features.iloc[:-test_days]\n",
    "val_df = df_features.iloc[-test_days:]\n",
    "\n",
    "# è¨­å®šç›®æ¨™æ¬„ä½åç¨±\n",
    "TARGET_COL = '0056_close_y'\n",
    "\n",
    "# æº–å‚™ X (ç‰¹å¾µ) å’Œ y (ç­”æ¡ˆ)\n",
    "# è¨˜å¾—å¾ X ä¸­æ‹¿æ‰ç­”æ¡ˆæ¬„ä½ï¼Œä»¥å…ç™¼ç”Ÿ Data Leakage (ä½œå¼Š)\n",
    "X_train = train_df.drop(columns=[TARGET_COL, 'date'], errors='ignore')\n",
    "y_train = train_df[TARGET_COL]\n",
    "\n",
    "X_val = val_df.drop(columns=[TARGET_COL, 'date'], errors='ignore')\n",
    "y_val = val_df[TARGET_COL]\n",
    "\n",
    "print(f\"è¨“ç·´é›†å¤§å°: {X_train.shape}, é©—è­‰é›†å¤§å°: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ¨¡å‹è¨“ç·´ (Model Training) - å¤¢å¹»éšŠä¼\n",
    "\n",
    "æˆ‘å€‘æ¡ç”¨ **é›†æˆå­¸ç¿’ (Ensemble Learning)** ç­–ç•¥ï¼Œçµåˆä¸‰ç¨®ä¸åŒç‰¹æ€§çš„æ¨¡å‹é€²è¡ŒæŠ•ç¥¨ï¼Œä»¥æå‡é æ¸¬çš„ç©©å®šæ€§èˆ‡æº–ç¢ºåº¦ï¼š\n",
    "1. **XGBoost**: æ¢¯åº¦æå‡æ¨¹ï¼Œæ“…é•·æ•æ‰è¤‡é›œéç·šæ€§é—œä¿‚ã€‚\n",
    "2. **Random Forest**: éš¨æ©Ÿæ£®æ—ï¼ŒæŠ—é›œè¨Šèƒ½åŠ›å¼·ï¼Œä¸æ˜“éæ“¬åˆã€‚\n",
    "3. **LightGBM**: è¼•é‡ç´šæ¢¯åº¦æå‡ï¼Œé‹ç®—é€Ÿåº¦å¿«ä¸”ç²¾åº¦é«˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šåˆ‡æ›å›å–®ä¸€ XGBoost æ¨¡å‹ï¼Œæ¸¬è©¦ Baseline...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# 1. å®šç¾©å€‹åˆ¥çš„å°ˆå®¶ (æ¨¡å‹)\n",
    "# --- å°ˆå®¶ A: XGBoost ---\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=1000, \n",
    "    learning_rate=0.05, \n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- å°ˆå®¶ B: Random Forest (éš¨æ©Ÿæ£®æ—) ---\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,  # æ¨¹çš„æ•¸é‡\n",
    "    max_depth=10,      # ä¸è¦å¤ªæ·±ï¼Œé¿å…æ­»èƒŒ\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- å°ˆå®¶ C: LightGBM (å¦ä¸€å€‹æ¢¯åº¦æå‡æ¨¹) ---\n",
    "lgbm = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1 # å®‰éœæ¨¡å¼\n",
    ")\n",
    "\n",
    "# 2. çµ„å»ºå¤¢å¹»éšŠä¼ (æŠ•ç¥¨å™¨)\n",
    "# weights å¯ä»¥è¨­å®šæ¬Šé‡ï¼Œä¾‹å¦‚è®“ XGBoost è¬›è©±å¤§è²ä¸€é»\n",
    "print(\"ğŸ¤ æ­£åœ¨çµ„å»ºé›†æˆæ¨¡å‹ (Ensemble Model)...\")\n",
    "model = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('xgb', xgb), \n",
    "        ('rf', rf), \n",
    "        ('lgbm', lgbm)\n",
    "    ],\n",
    "    weights=[2, 1, 1] # æ¬Šé‡: XGBä½”2ä»½ï¼Œå…¶ä»–å„1ä»½ (æ‚¨å¯ä»¥è‡ªå·±èª¿)\n",
    ")\n",
    "\n",
    "# 3. è¨“ç·´ (è·ŸåŸæœ¬ä¸€æ¨¡ä¸€æ¨£)\n",
    "print(\"ğŸš€ é–‹å§‹è¨“ç·´å¤¢å¹»éšŠä¼...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. é æ¸¬èˆ‡è©•ä¼°\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "# ç®—åˆ†æ•¸\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "mape = mean_absolute_percentage_error(y_val, preds)\n",
    "print(f\"ğŸ“Š é›†æˆæ¨¡å‹ MAPE: {mape:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç‰¹å¾µé‡è¦æ€§åˆ†æ (Feature Importance)\n",
    "\n",
    "ç‚ºäº†ç†è§£æ¨¡å‹çš„æ±ºç­–é‚è¼¯ï¼Œæˆ‘å€‘æå–äº†å„å€‹å­æ¨¡å‹çš„ç‰¹å¾µé‡è¦æ€§ä¸¦å–å¹³å‡ã€‚é€™èƒ½å¹«åŠ©æˆ‘å€‘å›ç­”ï¼š**ã€ŒAI è¦ºå¾—å“ªäº›æŒ‡æ¨™æœ€èƒ½é æ¸¬è‚¡åƒ¹ï¼Ÿã€**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XGBRegressor' object has no attribute 'named_estimators_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1. æŠ“å‡ºä¸‰ä½å°ˆå®¶\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m xgb_imp \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_estimators_\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m      6\u001b[0m rf_imp \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mnamed_estimators_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m      7\u001b[0m lgbm_imp \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mnamed_estimators_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgbm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfeature_importances_\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'XGBRegressor' object has no attribute 'named_estimators_'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- ç¶œåˆä¸‰ä½å°ˆå®¶çš„æ„è¦‹ ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 1. æŠ“å‡ºä¸‰ä½å°ˆå®¶\n",
    "xgb_imp = model.named_estimators_['xgb'].feature_importances_\n",
    "rf_imp = model.named_estimators_['rf'].feature_importances_\n",
    "lgbm_imp = model.named_estimators_['lgbm'].feature_importances_\n",
    "\n",
    "# 2. å¹³å‡ä»–å€‘çš„æ„è¦‹\n",
    "avg_importances = (xgb_imp + rf_imp + lgbm_imp) / 3\n",
    "indices = np.argsort(avg_importances)[::-1]\n",
    "\n",
    "# 3. ç•«åœ–\n",
    "top_n = 15\n",
    "plt.title(\"Top 15 Feature Importances (Ensemble Consensus)\")\n",
    "plt.bar(range(top_n), avg_importances[indices[:top_n]], align=\"center\", color='green')\n",
    "plt.xticks(range(top_n), X_train.columns[indices[:top_n]], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. é æ¸¬çµæœè¦–è¦ºåŒ– (Visualization)\n",
    "\n",
    "å°‡æ¨¡å‹å°æœ€å¾Œ 30 å¤©çš„é æ¸¬çµæœèˆ‡çœŸå¯¦è‚¡åƒ¹é€²è¡Œå°æ¯”ã€‚\n",
    "* **è—ç·š**: çœŸå¯¦è‚¡åƒ¹ (Actual)\n",
    "* **ç´…è™›ç·š**: æ¨¡å‹é æ¸¬ (Predicted)\n",
    "* **è§€å¯Ÿé‡é»**: ç´…ç·šæ˜¯å¦èƒ½è·Ÿä¸Šè—ç·šçš„è¶¨å‹¢è½‰æŠ˜ï¼Ÿæœ‰æ²’æœ‰å‡ºç¾åš´é‡çš„æ»¯å¾Œ (Lag)ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç•«åœ–æª¢æŸ¥ - é æ¸¬ vs çœŸå¯¦ (çœ‹æ¨¡å‹æœ‰æ²’æœ‰æŠ“åˆ°è¶¨å‹¢)\n",
    "preds = model.predict(X_val)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# ç•«å‡ºçœŸå¯¦è‚¡åƒ¹ (è—ç·š)\n",
    "plt.plot(val_df['date'], y_val, label='Actual Price', color='blue')\n",
    "# ç•«å‡ºé æ¸¬è‚¡åƒ¹ (ç´…è™›ç·š)\n",
    "plt.plot(val_df['date'], preds, label='Predicted Price', color='red', linestyle='--')\n",
    "\n",
    "plt.title(\"Prediction vs Actual (Last 30 Days)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ç‰¹å¾µç›¸é—œæ€§åˆ†æ (Correlation Heatmap)\n",
    "\n",
    "é€²ä¸€æ­¥åˆ†æå‰ 15 å¤§é‡è¦ç‰¹å¾µèˆ‡ç›®æ¨™è‚¡åƒ¹ (Target) ä¹‹é–“çš„ç›¸é—œæ€§ã€‚\n",
    "* **ç´…è‰² (æ­£ç›¸é—œ)**: æŒ‡æ¨™æ¼²ï¼Œè‚¡åƒ¹è·Ÿè‘—æ¼²ã€‚\n",
    "* **è—è‰² (è² ç›¸é—œ)**: æŒ‡æ¨™æ¼²ï¼Œè‚¡åƒ¹åè€Œè·Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- ä½¿ç”¨ Seaborn ç•«ç†±åŠ›åœ– ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# 1. æŒ‘é¸è¦è§€å¯Ÿçš„æ¬„ä½ (é€™è£¡åªé¸å‰ 15 å€‹é‡è¦ç‰¹å¾µ + è‚¡åƒ¹)\n",
    "# å‡è¨­ indices æ˜¯å‰›å‰›ç®—å‡ºä¾†çš„é‡è¦ç‰¹å¾µæ’åº\n",
    "top_features = X_train.columns[indices[:15]].tolist()\n",
    "plot_cols = top_features + [TARGET_COL]\n",
    "\n",
    "# 2. è¨ˆç®—ç›¸é—œä¿‚æ•¸çŸ©é™£\n",
    "corr_matrix = train_df[plot_cols].corr()\n",
    "\n",
    "# 3. ç•«ç†±åŠ›åœ–\n",
    "plt.title(\"Feature Correlation Heatmap\", fontsize=15)\n",
    "sns.heatmap(\n",
    "    corr_matrix, \n",
    "    annot=True,       # é¡¯ç¤ºæ•¸å­—\n",
    "    fmt=\".2f\",        # æ•¸å­—æ ¼å¼ (å°æ•¸é»å…©ä½)\n",
    "    cmap=\"coolwarm\",  # é¡è‰² (ç´…=æ­£ç›¸é—œ, è—=è² ç›¸é—œ)\n",
    "    linewidths=0.5,   # æ ¼å­é‚Šæ¡†\n",
    "    square=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æœ€çµ‚è©•ä¼°çµè«– (Conclusion)\n",
    "\n",
    "ç¶œåˆè©•ä¼°æ¨¡å‹åœ¨é©—è­‰é›†ä¸Šçš„è¡¨ç¾ï¼š\n",
    "* **RMSE (å‡æ–¹æ ¹èª¤å·®)**: é æ¸¬å€¼èˆ‡çœŸå¯¦å€¼å¹³å‡ç›¸å·®å¤šå°‘å…ƒã€‚\n",
    "* **MAPE (å¹³å‡çµ•å°ç™¾åˆ†æ¯”èª¤å·®)**: é æ¸¬èª¤å·®ä½”è‚¡åƒ¹çš„ç™¾åˆ†æ¯”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. è¨ˆç®— RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "\n",
    "# 2. è¨ˆç®— MAPE\n",
    "mape = mean_absolute_percentage_error(y_val, preds)\n",
    "\n",
    "print(f\"ğŸ“‰ é›†æˆæ¨¡å‹ RMSE (èª¤å·®é‡‘é¡): {rmse:.4f}\")\n",
    "print(f\"ğŸ“Š é›†æˆæ¨¡å‹ MAPE (èª¤å·®æ¯”ä¾‹): {mape:.2%}\")\n",
    "\n",
    "# --- ç°¡å–®åˆ¤è®€ ---\n",
    "print(\"-\" * 30)\n",
    "if mape < 0.05:\n",
    "    print(\"ğŸ† è©•èªï¼šå¤¢å¹»éšŠä¼å¤ªå¼·äº†ï¼èª¤å·®ä¸åˆ° 5%ï¼\")\n",
    "elif mape < 0.10:\n",
    "    print(\"âœ… è©•èªï¼šé‚„ä¸éŒ¯ï¼Œå¤šå€‹å°ˆå®¶æŠ•ç¥¨æœç„¶ç©©ï¼Œèª¤å·®åœ¨ 10% å…§ã€‚\")\n",
    "else:\n",
    "    print(\"âš ï¸ è©•èªï¼šå¥½åƒæ²’æœ‰æ¯”å–®ä¸€æ¨¡å‹æº–ï¼Ÿå¯èƒ½è¦æª¢æŸ¥æ˜¯ä¸æ˜¯æœ‰å“ªä½å°ˆå®¶(ä¾‹å¦‚éš¨æ©Ÿæ£®æ—)åœ¨æ‰¯å¾Œè…¿ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bad_file = 'experiments/history.csv'\n",
    "\n",
    "# æª¢æŸ¥æª”æ¡ˆåœ¨ä¸åœ¨\n",
    "if os.path.exists(bad_file):\n",
    "    # æª¢æŸ¥æ˜¯ä¸æ˜¯çœŸçš„å…¨ç©º (å¤§å°ç‚º 0)\n",
    "    if os.path.getsize(bad_file) == 0:\n",
    "        os.remove(bad_file)\n",
    "        print(f\"âœ… å·²åˆªé™¤æå£çš„ç©ºæª”æ¡ˆï¼š{bad_file}\")\n",
    "        print(\"ğŸ‘‰ ç¾åœ¨è«‹é‡æ–°åŸ·è¡Œä¸‹é¢é‚£ä¸€å€‹ 'å¯«å…¥ç´€éŒ„' çš„ Cellï¼Œå®ƒæœƒè‡ªå‹•å»ºç«‹ä¸€å€‹å¥åº·çš„æª”æ¡ˆï¼\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æª”æ¡ˆå­˜åœ¨ä¸”æœ‰å…§å®¹ï¼Œå¯èƒ½æ ¼å¼æœ‰éŒ¯ï¼Ÿå»ºè­°æ‰‹å‹•å»å·¦é‚Šè³‡æ–™å¤¾æŠŠå®ƒåˆªé™¤ã€‚\")\n",
    "else:\n",
    "    print(\"â“ æª”æ¡ˆæœ¬ä¾†å°±ä¸åœ¨ï¼Œæ‚¨å¯ä»¥ç›´æ¥è·‘ä¸‹é¢çš„ç¨‹å¼ç¢¼ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==========================================\n",
    "# # ğŸ“ å¯¦é©—ç´€éŒ„ (Experiment Logger)\n",
    "# # ==========================================\n",
    "# import experiment_logger\n",
    "\n",
    "# # 1. æŠŠæ‚¨é€™æ¬¡ç”¨çš„ã€Œåƒæ•¸ã€å¯«ä¸‹ä¾† (é€™å¾ˆé‡è¦ï¼Œä¸ç„¶ä»¥å¾Œæœƒå¿˜è¨˜é€™åˆ†æ•¸æ€ä¾†çš„)\n",
    "# # å¦‚æœæ˜¯ç”¨é›†æˆæ¨¡å‹ï¼š\n",
    "# model_params = \"Ensemble (XGB*2 + RF*1 + LGBM*1)\"\n",
    "# # å¦‚æœæ˜¯ç”¨å–®ä¸€æ¨¡å‹ï¼š\n",
    "# # model_params = \"XGBoost (n_estimators=1000, lr=0.05)\"\n",
    "\n",
    "# # 2. å¯«å…¥ç´€éŒ„\n",
    "# experiment_logger.log_experiment(\n",
    "#     model_name=\"Ensemble_v1\",      # æ‚¨å¯ä»¥è‡ªå·±å–å€‹ä»£è™Ÿ\n",
    "#     params=model_params,           # åƒæ•¸èªªæ˜\n",
    "#     rmse=rmse,                     # é€™æ˜¯ä¸Šé¢ç®—å‡ºä¾†çš„è®Šæ•¸\n",
    "#     mape=mape,                     # é€™æ˜¯ä¸Šé¢ç®—å‡ºä¾†çš„è®Šæ•¸\n",
    "#     note=\"å˜—è©¦äº†é›†æˆæ¨¡å‹ï¼Œæ•ˆæœä¼¼ä¹æ¯”å–®ä¸€æ¨¡å‹ç©©\" # å¯«çµ¦è‡ªå·±çœ‹çš„ç­†è¨˜\n",
    "# )\n",
    "\n",
    "# # 3. (é¸ç”¨) çœ‹ä¸€ä¸‹ç›®å‰çš„æ’è¡Œæ¦œ\n",
    "# print(\"\\nğŸ† æ­·å²æœ€ä½³ç´€éŒ„ï¼š\")\n",
    "# try:\n",
    "#     history = pd.read_csv('experiments/history.csv')\n",
    "#     # ä¾ç…§ MAPE ç”±å°åˆ°å¤§æ’ (è¶Šå°è¶Šæº–)\n",
    "#     print(history.sort_values('MAPE').head(3)[['Timestamp', 'Model', 'MAPE', 'Note']])\n",
    "# except:\n",
    "#     pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
