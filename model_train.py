# =================================================
# model_train.py - å¼·åŒ–ç‰ˆï¼šç›®æ¨™è½‰æ›èˆ‡è‡ªå‹•è·¯å¾‘åµæ¸¬
# =================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import datetime
# æ©Ÿå™¨å­¸ç¿’æ¨¡å‹èˆ‡èª¿åƒå·¥å…·
from xgboost import XGBRegressor
import optuna
# æ¨¡å‹è©•ä¼°æŒ‡æ¨™
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error 
import joblib

# è¨­å®š Seaborn é¢¨æ ¼
sns.set_style("whitegrid")

# =================================================
# 1. ç¹ªåœ–å°å¹«æ‰‹ (Visualizer Class)
# =================================================
class ModelVisualizer:
    """å°ˆé–€è² è²¬å¯¦é©—è¦–è¦ºåŒ–èˆ‡åœ–ç‰‡æ­¸æª”çš„é¡åˆ¥"""
    def __init__(self, timestamp, plot_dir):
        self.timestamp = timestamp
        self.plot_dir = plot_dir

    def plot_validation_curve(self, y_val, preds, val_score, mape):
        """åœ– A: é©—è­‰é›†é æ¸¬èµ°å‹¢åœ–"""
        plt.figure(figsize=(12, 5))
        plt.plot(y_val.index, y_val, label='Actual', color='blue', marker='o', markersize=4)
        plt.plot(y_val.index, preds, label='Predicted', color='red', linestyle='--', marker='x', markersize=4)
        plt.title(f"Validation Period: Actual vs Predicted\n(RMSE: {val_score:.4f}, MAPE: {mape:.2%})")
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        
        save_path = f"{self.plot_dir}/val_{self.timestamp}_rmse_{val_score:.2f}.png"
        plt.savefig(save_path)
        print(f"ğŸ“Š é©—è­‰èµ°å‹¢åœ–å·²å„²å­˜: {save_path}")

    def plot_feature_importance(self, model, feature_names):
        """åœ– B: ç‰¹å¾µé‡è¦æ€§åœ–"""
        plt.figure(figsize=(10, 6))
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1][:15]
        top_feat_names = feature_names[indices].tolist()
        
        plt.title(f"Top 15 Feature Importances_{self.timestamp}")
        plt.bar(range(len(top_feat_names)), importances[indices], color='green')
        plt.xticks(range(len(top_feat_names)), top_feat_names, rotation=90)
        plt.tight_layout()
        
        save_path = f"{self.plot_dir}/fi_{self.timestamp}.png"
        plt.savefig(save_path)
        print(f"ğŸ“Š ç‰¹å¾µé‡è¦æ€§åœ–å·²å„²å­˜: {save_path}")
        return top_feat_names

    def plot_correlation_heatmap(self, df, top_features, target_col):
        """åœ– C: ç›¸é—œä¿‚æ•¸ç†±åŠ›åœ–"""
        plt.figure(figsize=(12, 10))
        plot_cols = top_features + [target_col]
        # éæ¿¾æ‰ä¸åœ¨ df ä¸­çš„æ¬„ä½
        plot_cols = [c for c in plot_cols if c in df.columns]
        corr_matrix = df[plot_cols].corr()
        
        plt.title(f"Feature Correlation Heatmap_{self.timestamp}", fontsize=15)
        sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5, square=True)
        plt.tight_layout()
        
        save_path = f"{self.plot_dir}/heatmap_{self.timestamp}.png"
        plt.savefig(save_path)
        print(f"ğŸ“Š ç›¸é—œä¿‚æ•¸ç†±åŠ›åœ–å·²å„²å­˜: {save_path}")

# =================================================
# 2. ä¸»è¨“ç·´æµç¨‹
# =================================================
def train_and_predict(df_features, submission_file='sample_submission.csv', use_optuna=False):
    print("ğŸš€ [Training] å•Ÿå‹•æ¨¡å‹è¨“ç·´ç”Ÿç”¢ç·š...")
    
    # --- 1. è‡ªå‹•åµæ¸¬å·¥ä½œç›®éŒ„èˆ‡è·¯å¾‘ ---
    current_path = os.getcwd()
    # å¦‚æœæ˜¯åœ¨ archive ä¸‹åŸ·è¡Œï¼Œä¿®æ­£è·¯å¾‘å‰ç¶´
    is_in_archive = os.path.basename(current_path) == "archive"
    prefix = "" if is_in_archive else "archive/"
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")

    # ä¸å†å¼·åˆ¶åŠ  archive/ï¼Œè®“å®ƒæ ¹æ“šåŸ·è¡Œä½ç½®æ±ºå®š experiments è³‡æ–™å¤¾åœ¨å“ª
    plot_dir = "experiments/plots" 
    os.makedirs(plot_dir, exist_ok=True)
    viz = ModelVisualizer(timestamp, plot_dir)

    # æœå°‹è€ƒå·æª”æ¡ˆ
    possible_paths = [submission_file, "submission.csv", "sample_submission.csv", 
                      "../sample_submission.csv", "../data/sample_submission.csv"]
    found_submission = None
    for p in possible_paths:
        if os.path.exists(p):
            found_submission = p
            break
    
    if not found_submission:
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°è€ƒå·æª”æ¡ˆï¼Œè«‹æª¢æŸ¥è·¯å¾‘ã€‚ç›®å‰ç›®éŒ„: {current_path}")
    
    print(f"âœ… æˆåŠŸæ‰¾åˆ°è€ƒå·: {found_submission}")
    submit_df = pd.read_csv(found_submission)
    target_ids = submit_df['date'].values 
    target_col = '0056_close_y' 

    # --- 2. ç›®æ¨™å€¼è½‰æ› (é æ¸¬æ¼²è·Œ Diff) ---
    if 'date' in df_features.columns:
        df_features = df_features.set_index('date')
    
    # è¨ˆç®—æ¯æ—¥åƒ¹å·®ä½œç‚ºç›®æ¨™
    df_features['target_diff'] = df_features[target_col].diff()
    
    # åˆ‡åˆ†è€ƒè©¦é›†èˆ‡æ­·å²è³‡æ–™
    X_test = df_features.loc[df_features.index.isin(target_ids)].copy()
    X_train_full_raw = df_features.loc[~df_features.index.isin(target_ids)].dropna().copy()
    
    # ç´€éŒ„æ­·å²æœ€å¾Œä¸€å¤©çš„çœŸå¯¦åƒ¹æ ¼
    last_real_price = df_features.loc[~df_features.index.isin(target_ids), target_col].iloc[-1]
    
    y_train_full = X_train_full_raw['target_diff']
    # ç‰¹å¾µä¸­ç§»é™¤ç›®æ¨™åƒ¹æ ¼èˆ‡åƒ¹å·®
    X_train_full = X_train_full_raw.drop(columns=[target_col, 'target_diff'], errors='ignore')
    X_test = X_test.drop(columns=[target_col, 'target_diff'], errors='ignore')

    # åˆ‡åˆ†è¨“ç·´èˆ‡é©—è­‰
    split_idx = int(len(X_train_full) * 0.8)
    X_train, y_train = X_train_full.iloc[:split_idx], y_train_full.iloc[:split_idx]
    X_val, y_val = X_train_full.iloc[split_idx:], y_train_full.iloc[split_idx:]
    y_val_real_prices = X_train_full_raw.loc[X_val.index, target_col]

    # --- 3. æ¨¡å‹è¨“ç·´ (Optuna) ---
    if use_optuna:
        print("ğŸ¤– [Optuna] æœå°‹é æ¸¬ã€æ¼²è·Œå‹•èƒ½ã€çš„æœ€ä½³åƒæ•¸...")
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 1000, 3000),
                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05),
                'max_depth': trial.suggest_int('max_depth', 3, 8),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
                'random_state': 42, 'n_jobs': -1
            }
            m = XGBRegressor(**params)
            m.fit(X_train, y_train)
            # é©—è­‰æ™‚é‚„åŸåƒ¹æ ¼è¨ˆç®— RMSE
            p_diff = m.predict(X_val)
            p_real = X_train_full_raw[target_col].shift(1).loc[X_val.index] + p_diff
            return np.sqrt(mean_squared_error(y_val_real_prices, p_real))

        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=20)
        model = XGBRegressor(**study.best_params)
    else:
        model = XGBRegressor(n_estimators=1000, learning_rate=0.03, max_depth=6, random_state=42)

    # --- 4. é©—è­‰èˆ‡è¦–è¦ºåŒ– ---
    model.fit(X_train, y_train)
    val_p_diff = model.predict(X_val)
    # é‚„åŸåƒ¹æ ¼ï¼šå‰æ—¥åƒ¹æ ¼ + é æ¸¬æ¼²è·Œ
    val_p_real = X_train_full_raw[target_col].shift(1).loc[X_val.index] + val_p_diff
    
    score = np.sqrt(mean_squared_error(y_val_real_prices, val_p_real))
    mape = mean_absolute_percentage_error(y_val_real_prices, val_p_real)

    viz.plot_validation_curve(y_val_real_prices, val_p_real, score, mape)
    top_feats = viz.plot_feature_importance(model, X_train.columns)
    viz.plot_correlation_heatmap(df_features, top_feats, target_col)

    # --- 5. ç”¢å‡ºé æ¸¬ ---
    model.fit(X_train_full, y_train_full)
    test_diffs = model.predict(X_test)
    
    # ç´¯åŠ é‚„åŸè€ƒè©¦é›†åƒ¹æ ¼
    final_preds = []
    curr_p = last_real_price
    for d in test_diffs:
        curr_p += d
        final_preds.append(curr_p)

    submit_df[submit_df.columns[1]] = final_preds
    submit_df.to_csv('submission.csv', index=False)
    print(f"ğŸ‰ é æ¸¬å®Œæˆï¼RMSE: {score:.4f}")
    
    return model, score