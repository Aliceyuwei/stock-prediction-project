# =================================================
# 1. 
# =================================================
# å¥—ä»¶å°å…¥èˆ‡ç’°å¢ƒè¨­å®š
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import datetime
# æ©Ÿå™¨å­¸ç¿’æ¨¡å‹èˆ‡èª¿åƒå·¥å…·
from xgboost import XGBRegressor
import optuna
# æ¨¡å‹è©•ä¼°æŒ‡æ¨™
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error 

# è¨­å®š Seaborn é¢¨æ ¼
sns.set_style("whitegrid")

# =================================================
# 2. ç¹ªåœ–å°å¹«æ‰‹ (Visualizer Class)
# =================================================
class ModelVisualizer:
    """å°ˆé–€è² è²¬å¯¦é©—è¦–è¦ºåŒ–èˆ‡åœ–ç‰‡æ­¸æª”çš„é¡åˆ¥"""
    def __init__(self, timestamp, plot_dir):
        self.timestamp = timestamp
        self.plot_dir = plot_dir

    def plot_validation_curve(self, y_val, preds, val_score, mape):
        """åœ– A: é©—è­‰é›†é æ¸¬èµ°å‹¢åœ–"""
        plt.figure(figsize=(12, 5))
        plt.plot(y_val.index, y_val, label='Actual', color='blue', marker='o', markersize=4)
        plt.plot(y_val.index, preds, label='Predicted', color='red', linestyle='--', marker='x', markersize=4)
        plt.title(f"Validation Period: Actual vs Predicted\n(RMSE: {val_score:.4f}, MAPE: {mape:.2%})")
        plt.xticks(rotation=45)
        plt.legend()
        plt.tight_layout()
        
        save_path = f"{self.plot_dir}/val_{self.timestamp}_rmse_{val_score:.2f}.png"
        plt.savefig(save_path)
        print(f"ğŸ“Š é©—è­‰èµ°å‹¢åœ–å·²å„²å­˜: {save_path}")
        # plt.show()

    def plot_feature_importance(self, model, feature_names):
        """åœ– B: ç‰¹å¾µé‡è¦æ€§åœ–"""
        plt.figure(figsize=(10, 6))
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1][:15]
        top_feat_names = feature_names[indices].tolist()
        
        plt.title(f"Top 15 Feature Importances_{self.timestamp}")
        plt.bar(range(len(top_feat_names)), importances[indices], color='green')
        plt.xticks(range(len(top_feat_names)), top_feat_names, rotation=90)
        plt.tight_layout()
        
        save_path = f"{self.plot_dir}/fi_{self.timestamp}.png"
        plt.savefig(save_path)
        print(f"ğŸ“Š ç‰¹å¾µé‡è¦æ€§åœ–å·²å„²å­˜: {save_path}")
        # plt.show()
        return top_feat_names

    def plot_correlation_heatmap(self, df, top_features, target_col):
        """åœ– C: ç›¸é—œä¿‚æ•¸ç†±åŠ›åœ– (ä½¿ç”¨ Seaborn)"""
        plt.figure(figsize=(12, 10))
        # çµ„åˆå‰ 15 åç‰¹å¾µèˆ‡ç›®æ¨™åƒ¹æ ¼æ¬„ä½
        plot_cols = top_features + [target_col]
        corr_matrix = df[plot_cols].corr()
        
        plt.title(f"Feature Correlation Heatmap_{self.timestamp}", fontsize=15)
        sns.heatmap(
            corr_matrix, 
            annot=True, 
            fmt=".2f", 
            cmap="coolwarm", 
            linewidths=0.5, 
            square=True
        )
        plt.tight_layout()
        
        save_path = f"{self.plot_dir}/heatmap_{self.timestamp}.png"
        plt.savefig(save_path)
        print(f"ğŸ“Š ç›¸é—œä¿‚æ•¸ç†±åŠ›åœ–å·²å„²å­˜: {save_path}")
        # plt.show()

# =================================================
# 3. ä¸»è¨“ç·´æµç¨‹ (Main Training Logic)
# =================================================
def train_and_predict(df_features, submission_file='sample_submission.csv', use_optuna=False):
    """
    æ¥æ”¶ç‰¹å¾µå·¥ç¨‹å¾Œçš„è³‡æ–™ï¼Œè¨“ç·´ XGBoost æ¨¡å‹ã€‚
    åƒæ•¸ use_optuna=True æ™‚ï¼Œæœƒå•Ÿå‹•è‡ªå‹•èª¿åƒæ¨¡å¼ã€‚
    """
    print("ğŸš€ [Training] å•Ÿå‹•æ¨¡å‹è¨“ç·´ç”Ÿç”¢ç·š...")
    
    # --- åˆå§‹è¨­å®š ---
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
    plot_dir = "experiments/plots"
    os.makedirs(plot_dir, exist_ok=True)
    
    # åˆå§‹åŒ–ç¹ªåœ–å·¥å…·
    viz = ModelVisualizer(timestamp, plot_dir)

    # æª¢æŸ¥è€ƒå·è·¯å¾‘
    if not os.path.exists(submission_file):
        submission_file = 'submission.csv'
    
    # --- è³‡æ–™è™•ç† ---
    submit_df = pd.read_csv(submission_file)
    target_ids = submit_df['date'].values 

    # è¨­å®šç›®æ¨™æ¬„ä½ (ä¸»è§’)
    target_col = '0056_close_y' 

    # ç‚ºäº†æ–¹ä¾¿åˆ‡åˆ†ï¼Œå…ˆå°‡ date è¨­ç‚º index
    if 'date' in df_features.columns:
        df_features_indexed = df_features.set_index('date')
    else:
        df_features_indexed = df_features.copy()
    
    # --- åˆ‡åˆ† è¨“ç·´é›† (æ­·å²è³‡æ–™) vs è€ƒè©¦é›† (æœªä¾†è¦é æ¸¬çš„) ---
    X_test = df_features_indexed.loc[df_features_indexed.index.isin(target_ids)] # é€™æ˜¯æœ€å¾Œè¦äº¤å·çš„
    X_train_full = df_features_indexed.loc[~df_features_indexed.index.isin(target_ids)] # é€™æ˜¯æ‰€æœ‰çš„æ­·å²è³‡æ–™
    
    # åˆ†é›¢ç­”æ¡ˆ
    y_train_full = X_train_full[target_col]
    X_train_full = X_train_full.drop(columns=[target_col], errors='ignore')
    X_test = X_test.drop(columns=[target_col], errors='ignore')
    
    print(f"ğŸ“š æ­·å²è³‡æ–™ç¸½æ•¸: {X_train_full.shape}")
    print(f"ğŸ“ é æ¸¬è³‡æ–™é›†: {X_test.shape}")
    # åˆ‡åˆ†è¨“ç·´èˆ‡é©—è­‰é›†
    split_point = int(len(X_train_full) * 0.8)
    X_train, y_train = X_train_full.iloc[:split_point], y_train_full.iloc[:split_point]
    X_val, y_val = X_train_full.iloc[split_point:], y_train_full.iloc[split_point:]

    print(f"   ğŸ‘‰ å¯¦éš›è¨“ç·´ç”¨: {X_train.shape}, é©—è­‰ç”¨: {X_val.shape}")
    
    # --- æ¨¡å‹å®šç¾©èˆ‡èª¿åƒ ---
    if use_optuna:
        print("ğŸ¤– [Optuna] å•Ÿå‹•è‡ªå‹•åŒ–åƒæ•¸æœå°‹...")
        def objective(trial):
            # è®“ AI éš¨æ©Ÿå˜—è©¦é€™äº›åƒæ•¸
            params = {
                # 1. ã€æ ¸å¿ƒæˆ°è¡“ã€‘ä»¥æ…¢æ‰“å¿«ï¼šæ›´å¤šæ¨¹ï¼Œä½†æ¯æ£µæ¨¹å­¸å°‘ä¸€é»
                'n_estimators': trial.suggest_int('n_estimators', 1500, 3500), # æ‹‰é«˜ä¸Šé™
                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.05), # é™ä½å­¸ç¿’ç‡
                
                # 2. æ·±åº¦æ§åˆ¶ï¼šçµ¦å®ƒä¸€é»é»ç©ºé–“ï¼Œå¾ 3-6 æ”¾å¯¬åˆ° 3-7
                'max_depth': trial.suggest_int('max_depth', 3, 7),
                
                # 3. æ­£å‰‡åŒ– (ç¶­æŒå‰›æ‰çš„ Log æ¨¡å¼ï¼Œé€™å¾ˆæ£’)
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),
                
                # 4. ç¨å¾®èª¿ä½ min_child_weight (åŸæœ¬ 1-10 æœ‰é»å¤ªåš´æ ¼ï¼Œæ”¹ 1-5)
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),
                
                # å…¶ä»–ç¶­æŒä¸è®Š
                'subsample': trial.suggest_float('subsample', 0.6, 0.85),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.85),
                'n_jobs': -1,
                'random_state': 42
            }
            
            # è¨“ç·´ä¸€å€‹è‡¨æ™‚æ¨¡å‹
            temp_model = XGBRegressor(**params)
            temp_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)

            # ç®—åˆ†æ•¸
            return np.sqrt(mean_squared_error(y_val, temp_model.predict(X_val)))

        # é–‹å§‹è·‘ 20 æ¬¡å¯¦é©— (ä½ å¯ä»¥æ”¹ n_trials=50 æœƒæ›´æº–)
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=20)
        
        print(f"ğŸ‰ æ‰¾åˆ°æœ€ä½³åƒæ•¸: {study.best_params}")
        print(f"ğŸ“‰ æœ€ä½³åˆ†æ•¸ (RMSE): {study.best_value:.4f}")
        val_score = study.best_value
        model = XGBRegressor(**study.best_params, n_jobs=-1, random_state=42)
    else:
        print("ğŸ¤ ä½¿ç”¨æ‰‹å‹•é è¨­åƒæ•¸æ¨¡å¼...")

        model = XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6, random_state=42, n_jobs=-1)
        model.fit(X_train, y_train)
        val_score = np.sqrt(mean_squared_error(y_val, model.predict(X_val)))

    # --- è¦–è¦ºåŒ–è¨ºæ–· (æ¡ç”¨ ModelVisualizer) ---
    model.fit(X_train, y_train)
    preds = model.predict(X_val)
    mape = mean_absolute_percentage_error(y_val, preds)

    # ä¾åºåŸ·è¡Œç¹ªåœ–ä»»å‹™ A, B, C
    viz.plot_validation_curve(y_val, preds, val_score, mape)
    top_feats = viz.plot_feature_importance(model, X_train.columns)
    viz.plot_correlation_heatmap(df_features, top_feats, target_col)

    # --- æœ€çµ‚ç”¢å‡º ---
    print("ğŸš€ ä½¿ç”¨å®Œæ•´æ­·å²è³‡æ–™é‡æ–°è¨“ç·´ (Full Retrain)...")
    model.fit(X_train_full, y_train_full)
    
    print("ğŸ”® æ­£åœ¨é€²è¡Œæœ€çµ‚é æ¸¬...")
    predictions = model.predict(X_test)

    pred_df = pd.DataFrame({'date': X_test.index, 'prediction': predictions})
    final_submission = submit_df[['date']].merge(pred_df, on='date', how='left')
    target_submit_col = [c for c in submit_df.columns if c != 'date'][0]
    final_submission[target_submit_col] = final_submission['prediction']
    final_submission[['date', target_submit_col]].to_csv('submission.csv', index=False)
    
    print(f"ğŸ‰ è€ƒå·å·²å¡«å¯«å®Œæˆ: submission.csv")
    return model, val_score