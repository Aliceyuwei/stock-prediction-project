# =================================================
# 1. è³‡æ–™è™•ç†èˆ‡æ•¸å­¸é‹ç®—
# =================================================
import pandas as pd
import numpy as np

# =================================================
# 2. æ©Ÿå™¨å­¸ç¿’æ¨¡å‹èˆ‡èª¿åƒå·¥å…·
# =================================================
from xgboost import XGBRegressor
import optuna
# å‚™è¨»ï¼šå¦‚æœä½ ä¹‹å¾Œè¦æ¢å¾©ä½¿ç”¨é›†æˆæ¨¡å‹ (Voting)ï¼Œè¨˜å¾—è¦æŠŠ RandomForestRegressor, 
# LGBMRegressor, VotingRegressor åŠ å›ä¾†

# =================================================
# 3. æ¨¡å‹è©•ä¼°æŒ‡æ¨™
# =================================================
from sklearn.metrics import mean_squared_error 

def train_and_predict(df_features, submission_file='sample_submission.csv', use_optuna=False):
    """
    æ¥æ”¶ç‰¹å¾µå·¥ç¨‹å¾Œçš„è³‡æ–™ï¼Œè¨“ç·´ XGBoost æ¨¡å‹ã€‚
    åƒæ•¸ use_optuna=True æ™‚ï¼Œæœƒå•Ÿå‹•è‡ªå‹•èª¿åƒæ¨¡å¼ã€‚
    """
    print("ğŸš€ [Training] å•Ÿå‹•æ¨¡å‹è¨“ç·´ç”Ÿç”¢ç·š...")
    
    # =================================================
    # 1. æº–å‚™è³‡æ–™èˆ‡å®šç¾©ç›®æ¨™
    # =================================================
    # è®€å–è€ƒå·ï¼Œç¢ºèªè¦é æ¸¬å“ªäº› ID (Date)
    submit_df = pd.read_csv(submission_file)
    target_ids = submit_df['date'].values 

    # è¨­å®šç›®æ¨™æ¬„ä½ (ä¸»è§’)
    target_col = '0056_close_y' 

    # ç‚ºäº†æ–¹ä¾¿åˆ‡åˆ†ï¼Œå…ˆå°‡ date è¨­ç‚º index
    if 'date' in df_features.columns:
        df_features = df_features.set_index('date')
    
    # --- åˆ‡åˆ† è¨“ç·´é›† (æ­·å²è³‡æ–™) vs è€ƒè©¦é›† (æœªä¾†è¦é æ¸¬çš„) ---
    X_test = df_features.loc[df_features.index.isin(target_ids)] # é€™æ˜¯æœ€å¾Œè¦äº¤å·çš„
    X_train_full = df_features.loc[~df_features.index.isin(target_ids)] # é€™æ˜¯æ‰€æœ‰çš„æ­·å²è³‡æ–™
    
    # åˆ†é›¢ç­”æ¡ˆ
    y_train_full = X_train_full[target_col]
    X_train_full = X_train_full.drop(columns=[target_col], errors='ignore')
    X_test = X_test.drop(columns=[target_col], errors='ignore')
    
    print(f"ğŸ“š æ­·å²è³‡æ–™ç¸½æ•¸: {X_train_full.shape}")
    print(f"ğŸ“ é æ¸¬è³‡æ–™é›†: {X_test.shape}")

    # =================================================
    # 2. å…§éƒ¨é©—è­‰ (ç‚ºäº†ç®—å‡ºåˆ†æ•¸)
    # =================================================
    # åˆ‡å‡ºå¾Œ 20% çš„è³‡æ–™ç•¶ä½œé©—è­‰é›† (Validation Set)
    split_point = int(len(X_train_full) * 0.8)

    X_train = X_train_full.iloc[:split_point]
    y_train = y_train_full.iloc[:split_point]

    X_val = X_train_full.iloc[split_point:]
    y_val = y_train_full.iloc[split_point:]
    
    print(f"   ğŸ‘‰ å¯¦éš›è¨“ç·´ç”¨: {X_train.shape}, é©—è­‰ç”¨: {X_val.shape}")

    # =================================================
    # 3. å®šç¾©æ¨¡å‹ (åˆ†ç‚ºä¸€èˆ¬æ¨¡å¼ vs è‡ªå‹•èª¿åƒæ¨¡å¼)
    # =================================================
    
    if use_optuna:
        print("ğŸ¤– [Optuna] å•Ÿå‹•ï¼æ­£åœ¨å°‹æ‰¾æœ€å¼·åƒæ•¸ (é€™æœƒèŠ±ä¸€é»æ™‚é–“)...")
        
        # å®šç¾©çµ¦ Optuna çš„è€ƒè©¦è¦å‰‡
        def objective(trial):
            # è®“ AI éš¨æ©Ÿå˜—è©¦é€™äº›åƒæ•¸
            params = {
                # é™åˆ¶æ¨¹çš„æ·±åº¦ï¼Œä¸è®“å®ƒå¤ªæ·± (åŸæœ¬ max 10 å¤ªæ·±äº†)
                'n_estimators': trial.suggest_int('n_estimators', 500, 1500),
                'max_depth': trial.suggest_int('max_depth', 3, 6), 
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
                # å¢åŠ æ­£å‰‡åŒ–æ‡²ç½° (æ‡²ç½°å¤ªè¤‡é›œçš„æ¨¡å‹)
                'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10.0),
                'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10.0),
                'subsample': trial.suggest_float('subsample', 0.6, 0.85),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.85),
                'n_jobs': -1,
                'random_state': 42
            }
            
            # è¨“ç·´ä¸€å€‹è‡¨æ™‚æ¨¡å‹
            temp_model = XGBRegressor(**params)
            temp_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
            
            # ç®—åˆ†æ•¸
            preds = temp_model.predict(X_val)
            rmse = np.sqrt(mean_squared_error(y_val, preds))
            return rmse

        # é–‹å§‹è·‘ 20 æ¬¡å¯¦é©— (ä½ å¯ä»¥æ”¹ n_trials=50 æœƒæ›´æº–)
        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=20)
        
        print(f"ğŸ‰ æ‰¾åˆ°æœ€ä½³åƒæ•¸: {study.best_params}")
        print(f"ğŸ“‰ æœ€ä½³åˆ†æ•¸ (RMSE): {study.best_value:.4f}")
        
        # ä½¿ç”¨æ‰¾åˆ°çš„æœ€å¼·åƒæ•¸å»ºç«‹æ¨¡å‹
        best_params = study.best_params
        model = XGBRegressor(**best_params, n_jobs=-1, random_state=42)
        
    else:
        # é€™æ˜¯ä½ åŸæœ¬çš„æ‰‹å‹•è¨­å®š (Fallback)
        print("ğŸ¤ ä½¿ç”¨é è¨­åƒæ•¸æ¨¡å¼...")
        model = XGBRegressor(
            n_estimators=1000, 
            learning_rate=0.05, 
            max_depth=6,
            random_state=42,
            n_jobs=-1
        )

    # =================================================
    # 4. è¨“ç·´èˆ‡è©•åˆ†
    # =================================================
    print("ğŸš€ ä½¿ç”¨å®Œæ•´æ­·å²è³‡æ–™é‡æ–°è¨“ç·´ (Full Retrain)...")
    model.fit(X_train_full, y_train_full)
    
    # ç®—ä¸€ä¸‹é©—è­‰åˆ†æ•¸ (å¦‚æœæ˜¯ Optuna æ¨¡å¼ï¼Œç›´æ¥ç”¨æœ€ä½³åˆ†æ•¸)
    if use_optuna:
        val_score = study.best_value
    else:
        # æ‰‹å‹•æ¨¡å¼è¦é‡ç®—ä¸€æ¬¡
        temp_model = model # é€™è£¡åªæ˜¯ä¸€å€‹è¿‘ä¼¼ï¼Œå¯¦éš›ä¸Š Full Retrain å¾Œç„¡æ³•ç®— Val Scoreï¼Œæ‰€ä»¥æˆ‘å€‘æ²¿ç”¨ä¹‹å‰çš„æ¦‚å¿µ
        # ç‚ºäº†ç°¡å–®èµ·è¦‹ï¼Œæˆ‘å€‘é‡æ–°ç”¨ 80/20 è¨“ç·´ä¸€æ¬¡ä¾†æ‹¿åˆ†æ•¸ï¼Œæˆ–æ˜¯ç›´æ¥å›å‚³ 0
        # é€™è£¡ç°¡å–®è™•ç†ï¼šå›å‚³æœ€å¾Œä¸€æ¬¡é©—è­‰çš„åˆ†æ•¸
        model_for_score = XGBRegressor(**model.get_params())
        model_for_score.fit(X_train, y_train)
        val_preds = model_for_score.predict(X_val)
        val_score = np.sqrt(mean_squared_error(y_val, val_preds))
        print(f"âœ… æ‰‹å‹•æ¨¡å¼é©—è­‰åˆ†æ•¸: {val_score:.4f}")

    # =================================================
    # 5. é æ¸¬èˆ‡å­˜æª”
    # =================================================
    print("ğŸ”® æ­£åœ¨é€²è¡Œæœ€çµ‚é æ¸¬...")
    predictions = model.predict(X_test)
    
    pred_df = pd.DataFrame({'date': X_test.index, 'prediction': predictions})
    final_submission = submit_df[['date']].merge(pred_df, on='date', how='left')
    target_submit_col = [c for c in submit_df.columns if c != 'date'][0]
    final_submission[target_submit_col] = final_submission['prediction']
    
    output_filename = 'submission.csv'
    final_submission[['date', target_submit_col]].to_csv(output_filename, index=False)
    
    print(f"ğŸ‰ è€ƒå·å·²å¡«å¯«å®Œæˆ: {output_filename}")
    
    return model, val_score